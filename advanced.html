<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Advanced functionalities in a benchmark &#8212; benchopt 1.3.2.dev31 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="_static/style.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-rendered-html.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="What’s new" href="whats_new.html" />
    <link rel="prev" title="BenchOpt configuration" href="config.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="_static/js/jquery-1.12.4.min.js"></script>
<script type="text/javascript" src="_static/js/jquery-fix.js"></script>
<script type="text/javascript" src="_static/bootstrap-3.4.1/js/bootstrap.min.js"></script>
<script type="text/javascript" src="_static/bootstrap-sphinx.js"></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          benchopt</a>
        <span class="navbar-text navbar-version pull-left"><b>1.3.2.dev31</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="cli.html">CLI</a></li>
                <li><a href="api.html">API</a></li>
                <li><a href="https://benchopt.github.io/results">Results</a></li>
                <li><a href="whats_new.html">What's new</a></li>
                <li><a href="https://github.com/benchopt/benchopt">GitHub</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command line interface (CLI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="how.html">Write a benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="publish.html">Publish benchmark results</a></li>
<li class="toctree-l1"><a class="reference internal" href="config.html">BenchOpt configuration</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Advanced functionalities in a benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="whats_new.html">What’s new</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/benchopt/benchopt">Fork benchopt on Github</a></li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="col-md-3">
        <div id="sidebar" class="bs-sidenav" role="complementary"><ul>
<li><a class="reference internal" href="#">Advanced functionalities in a benchmark</a><ul>
<li><a class="reference internal" href="#running-the-benchmark-on-a-slurm-cluster">Running the benchmark on a SLURM cluster</a></li>
<li><a class="reference internal" href="#skipping-a-solver-for-a-given-problem">Skipping a solver for a given problem</a></li>
<li><a class="reference internal" href="#changing-the-strategy-to-grow-the-computational-budget-stop-val">Changing the strategy to grow the computational budget (<code class="code docutils literal notranslate"><span class="pre">stop_val</span></code>)</a></li>
<li><a class="reference internal" href="#reusing-some-code-in-a-benchmark">Reusing some code in a benchmark</a></li>
<li><a class="reference internal" href="#caching-pre-compilation-and-warmup-effects">Caching pre-compilation and warmup effects</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    <div class="body col-md-9 content" role="main">
      
  <section id="advanced-functionalities-in-a-benchmark">
<span id="advanced"></span><h1>Advanced functionalities in a benchmark<a class="headerlink" href="#advanced-functionalities-in-a-benchmark" title="Permalink to this headline">¶</a></h1>
<p>This page intends to list some advanced functionality
to make it easier to use the benchmark.</p>
<section id="running-the-benchmark-on-a-slurm-cluster">
<span id="slurm-run"></span><h2>Running the benchmark on a SLURM cluster<a class="headerlink" href="#running-the-benchmark-on-a-slurm-cluster" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">benchopt</span></code> also allows easily running the benchmark in parallel on a SLURM
cluster. To install the necessary dependencies, please run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt1:before {
  content: "$ ";
}
</style><span class="prompt1">pip<span class="w"> </span>install<span class="w"> </span>benchopt<span class="o">[</span>slurm<span class="o">]</span></span>
<span class="prompt1"></span>
<span class="prompt1"><span class="c1"># Or for dev install</span></span>
<span class="prompt1">pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.<span class="o">[</span>slurm<span class="o">]</span></span>
</pre></div></div><p>Note that for some clusters with shared python installation, it is necessary
to call <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">--user</span></code> to install the packages in the user space and
not in the the system one.</p>
<p>Using the <code class="docutils literal notranslate"><span class="pre">--slurm</span></code> option for <code class="docutils literal notranslate"><span class="pre">benchopt</span> <span class="pre">run</span></code>, one can pass a config file
used to setup the SLURM jobs. This file is a YAML file that can contain any key
to be passed to the <a class="reference external" href="https://github.com/facebookincubator/submitit/blob/main/submitit/slurm/slurm.py#L386"><code class="docutils literal notranslate"><span class="pre">update_parameters</span></code></a> method of <a class="reference external" href="https://github.com/facebookincubator/submitit/blob/main/submitit/slurm/slurm.py#L214"><code class="docutils literal notranslate"><span class="pre">submitit.SlurmExecutor</span></code></a>.
Hereafter is an example of such config file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">slurm_time</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">01:00:00</span><span class="w">        </span><span class="c1"># max runtime 1 hour</span>
<span class="nt">slurm_gres</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpu:1</span><span class="w">           </span><span class="c1"># requires 1 GPU per job</span>
<span class="nt">slurm_additional_parameters</span><span class="p">:</span>
<span class="w">  </span><span class="nt">ntasks</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w">                 </span><span class="c1"># Number of tasks per job</span>
<span class="w">  </span><span class="nt">cpus-per-task</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span><span class="w">         </span><span class="c1"># requires 10 CPUs per job</span>
<span class="w">  </span><span class="nt">qos</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">QOS_NAME</span><span class="w">             </span><span class="c1"># Queue used for the jobs</span>
<span class="w">  </span><span class="nt">distribution</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">block:block</span><span class="w"> </span><span class="c1"># Distribution on the node&#39;s architectures</span>
<span class="w">  </span><span class="nt">account</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ACC@NAME</span><span class="w">         </span><span class="c1"># Account for the jobs</span>
<span class="nt">slurm_setup</span><span class="p">:</span><span class="w">  </span><span class="c1"># sbatch script commands added before the main job</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;#SBATCH</span><span class="nv"> </span><span class="s">-C</span><span class="nv"> </span><span class="s">v100-16g&#39;</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">module purge</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">module load cuda/10.1.2 cudnn/7.6.5.32-cuda-10.1 nccl/2.5.6-2-cuda</span>
</pre></div>
</div>
<p>Using such option, each configuration of <code class="docutils literal notranslate"><span class="pre">(dataset,</span> <span class="pre">objective,</span> <span class="pre">solver)</span></code> with
unique parameters are launched as a separated job in a job-array on the SLURM
cluster. Note that by default, no limitation is used on the number of
simultaneous jobs that are run.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">slurm_time</span></code> is not set in the config file, <code class="docutils literal notranslate"><span class="pre">benchopt</span></code> uses by default
the value of <code class="docutils literal notranslate"><span class="pre">--timeout</span></code> multiplied by <code class="docutils literal notranslate"><span class="pre">1.5</span></code> for each job.
Note that the logs of each benchmark run can be found in <code class="docutils literal notranslate"><span class="pre">./benchopt_run/</span></code>.</p>
<p>As we rely on <code class="docutils literal notranslate"><span class="pre">joblib.Memory</span></code> for caching the results, the cache should work
exactly as if you were running the computation sequentially, as long as you have
a shared file-system between the nodes used for the computations.</p>
</section>
<section id="skipping-a-solver-for-a-given-problem">
<span id="skiping-solver"></span><h2>Skipping a solver for a given problem<a class="headerlink" href="#skipping-a-solver-for-a-given-problem" title="Permalink to this headline">¶</a></h2>
<p>Some solvers might not be able to run with all the datasets present
in a benchmark. This is typically the case when some datasets are
represented using sparse data or for datasets that are too large.</p>
<p>In this cases, a solver can be skipped at runtime, depending on the
characteristic of the objective. In order to define for which cases
a solver should be skip, the user needs to implement a method
<a class="reference internal" href="generated/benchopt.BaseSolver.html#benchopt.BaseSolver.skip" title="benchopt.BaseSolver.skip"><code class="xref py py-func docutils literal notranslate"><span class="pre">skip()</span></code></a> in the solver class that will take
the input as the <a class="reference internal" href="generated/benchopt.BaseSolver.html#benchopt.BaseSolver.set_objective" title="benchopt.BaseSolver.set_objective"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_objective()</span></code></a> method.
This method should return a boolean value that evaluate to <code class="docutils literal notranslate"><span class="pre">True</span></code>
if the solver should be skipped, and a string giving the reason of
why the solver is skipped, for display purposes. For instance,
for a solver where the objective is set with keys <cite>X, y, reg</cite>,
we get</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Solver</span><span class="p">(</span><span class="n">BaseSolver</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="nf">skip</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">sparse</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;solver does not support sparse data X.&quot;</span>

        <span class="k">if</span> <span class="n">reg</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;solver does not work with reg=0&quot;</span>

        <span class="k">return</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span>
</pre></div>
</div>
</section>
<section id="changing-the-strategy-to-grow-the-computational-budget-stop-val">
<span id="sampling-strategy"></span><h2>Changing the strategy to grow the computational budget (<code class="code docutils literal notranslate"><span class="pre">stop_val</span></code>)<a class="headerlink" href="#changing-the-strategy-to-grow-the-computational-budget-stop-val" title="Permalink to this headline">¶</a></h2>
<p>Benchopt varies the computational budget by varying either the number
of iterations or the tolerance given to the method. The default policy is
to vary these two quantities exponentially between two evaluations of the
objective. However, in some cases, this exponential growth might hide some
effects, or might not be adapted to a given solver.</p>
<p>The way this value is changed can be specified for each solver by
implementing a static <code class="docutils literal notranslate"><span class="pre">get_next</span></code> method in the <code class="docutils literal notranslate"><span class="pre">Solver</span></code> class.
This method takes as input the previous value where the objective
function has been logged, and outputs the next one. For instance,
if a solver needs to be evaluated every 10 iterations, we would have</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Solver</span><span class="p">(</span><span class="n">BaseSolver</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_next</span><span class="p">(</span><span class="n">stop_val</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">stop_val</span> <span class="o">+</span> <span class="mi">10</span>
</pre></div>
</div>
</section>
<section id="reusing-some-code-in-a-benchmark">
<span id="benchmark-utils-import"></span><h2>Reusing some code in a benchmark<a class="headerlink" href="#reusing-some-code-in-a-benchmark" title="Permalink to this headline">¶</a></h2>
<p>In some situations, multiple solvers need to have access to the same
functions. As a benchmark is not structured as proper python packages
but imported dynamically to avoid installation issues, we resort to
a special way of importing modules and functions defined for a benchmark.</p>
<p>First, all code that need to be imported should be placed under
<code class="docutils literal notranslate"><span class="pre">BENCHMARK_DIR/benchmark_utils/</span></code>, as described here:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>my_benchmark/
├── objective.py  # contains the definition of the objective
├── datasets/
├── solvers/
└── benchmark_utils/
    ├── __init__.py
    ├── helper1.py  # some helper
    └─── helper_module  # a submodule
        ├── __init__.py
        └── submodule1.py  # some more helpers
</pre></div>
</div>
<p>Then, these modules and packages can be imported as a regular package, i.e.,
.. code-block:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">benchopt</span> <span class="kn">import</span> <span class="n">safe_import_context</span>

<span class="k">with</span> <span class="n">safe_import_context</span><span class="p">()</span> <span class="k">as</span> <span class="n">import_ctx</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">benchmark_utils</span> <span class="kn">import</span> <span class="n">helper1</span>
    <span class="kn">from</span> <span class="nn">benchmark_utils.helper1</span> <span class="kn">import</span> <span class="n">func1</span>
    <span class="kn">from</span> <span class="nn">benchmark_utils.helper_module.submodule1</span> <span class="kn">import</span> <span class="n">func2</span>
</pre></div>
</div>
</section>
<section id="caching-pre-compilation-and-warmup-effects">
<span id="precompilation"></span><h2>Caching pre-compilation and warmup effects<a class="headerlink" href="#caching-pre-compilation-and-warmup-effects" title="Permalink to this headline">¶</a></h2>
<p>For some solvers, such as solver relying on just-in-time compilation with
<code class="docutils literal notranslate"><span class="pre">numba</span></code> or <code class="docutils literal notranslate"><span class="pre">jax</span></code>, the first iteration might be longer due to “warmup”
effects. To avoid having such effect in the benchmark results, it is usually
advised to call the solver once before running the benchmark, in the
<code class="docutils literal notranslate"><span class="pre">Solver.set_objective</span></code> method. For solvers with <code class="docutils literal notranslate"><span class="pre">stopping_strategy</span></code> in
<code class="docutils literal notranslate"><span class="pre">{'tolerance',</span>&#160; <span class="pre">'iteration'}</span></code>, simply calling the <code class="docutils literal notranslate"><span class="pre">Solver.run</span></code> with a
simple enough value is usually enough. For solvers with <code class="docutils literal notranslate"><span class="pre">stopping_strategy</span></code>
set to <code class="docutils literal notranslate"><span class="pre">'callback'</span></code>, it is possible to call <code class="docutils literal notranslate"><span class="pre">Solver.run_once</span></code>, which will
call the <code class="docutils literal notranslate"><span class="pre">run</span></code> method with a simple callback that does not compute the
objective value and stops after <code class="docutils literal notranslate"><span class="pre">n_iter</span></code> calls to callback (default to 1).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Solver</span><span class="p">(</span><span class="n">BaseSolver</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">set_objective</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">objective</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="c1"># Cache pre-compilation and other one-time setups that should</span>
        <span class="c1"># not be included in the benchmark timing.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># For stopping_strategy == &#39;iteration&#39; | &#39;tolerance&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_once</span><span class="p">()</span>  <span class="c1"># For stopping_strategy == &#39;callback&#39;</span>
</pre></div>
</div>
</section>
</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2020-2022, Benchopt contributors.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.5.0.<br/>
    </p>
  </div>
</footer>
  </body>
</html>